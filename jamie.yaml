general:
  results_file: out2.txt
  conv_mode: matrix
  save_adv_example: true
  save_output: true
  show_adv_example: true
  complete_verifier: skip
  # return_optimized_model: true
# This is an example configuration file that contains most useful parameter settings.
model:
  # Model is defined as simple_conv_model() in custom_model_data.py. The function simple_conv_model() will be called with arguments in_channel=3, out_dim=10.
  name: Customized("custom_model_data", "get_model")
  # Path to model checkpoint.
  path: alpha-beta-CROWN/complete_verifier/trained_models/seed=1,epochs=1,lambda_entropy=0.025,model=MLP,learning_rate=0.001,checkpoint_freq=10,rebalance_freq=5,jitter=1.5,checkpoint=1.pth
data:
  dataset: Customized("custom_model_data", "asset_dataset")
  num_outputs: 4
  start: 0
  end: 100

specification:
  epsilon: 1  # 2./255.
  robustness_type: custom
  delta: 0.2
solver:
  batch_size: 128
  # beta-crown:
  #   iteration: 1

  alpha-crown:
    iteration: 200

attack:
  pgd_order: before
  pgd_steps: 100 
  pgd_restarts: 60
  pgd_early_stop: true
  # adv_saver: custom_adv_saver

# bab:
#   timeout: 20
#   vanilla_crown: true
#   max_iterations: 4




# attack:
#   pgd_restarts: 50
# solver:
#   batch_size: 4096
#   beta-crown:
#     iteration: 20
# bab:
#   timeout: 20
#   # dataset: CIFAR  # Dataset name. This is just the standard CIFAR-10 test set defined in the "load_verification_dataset()" function in utils.py
#   # mean: [0.4914, 0.4822, 0.4465]  # Mean for normalization.
#   # std: [0.2471, 0.2435, 0.2616]  # Std for normalization.
#   # start: 0  # First example to verify in dataset.results_file: out.txt
#   # end: 5  # Last example to verify in dataset. We verify 100 examples in this test.
# specification:
#   norm: .inf  # Linf norm (can also be 2 or 1).
#   epsilon: 1  # epsilon=2./255.
# attack:  # Currently attack is only implemented for Linf norm.
#   pgd_steps: 100  # Increase for a stronger attack. A PGD attack will be used before verification to filter on non-robust data examples.
#   pgd_restarts: 30  # Increase for a stronger attack.
# solver:
#   batch_size: 2048  # Number of subdomains to compute in parallel in bound solver. Decrease if you run out of memory.
#   alpha-crown:
#     iteration: 3   # Number of iterations for alpha-CROWN optimization. Alpha-CROWN is used to compute all intermediate layer bounds before branch and bound starts.
#     lr_alpha: 0.1    # Learning rate for alpha in alpha-CROWN. The default (0.1) is typically ok.
#   beta-crown:
#     lr_alpha: 0.01  # Learning rate for optimizing the alpha parameters, the default (0.01) is typically ok, but you can try to tune this parameter to get better lower bound.
#     lr_beta: 0.05  # Learning rate for optimizing the beta parameters, the default (0.05) is typically ok, but you can try to tune this parameter to get better lower bound.
#     iteration: 3  # Number of iterations for beta-CROWN optimization. 20 is often sufficient, 50 or 100 can also be used.
# bab:
#   timeout: 120  # Timeout threshold for branch and bound. Increase for verifying more points.
#   branching:  # Parameters for branching heuristics.
#     reduceop: min  # Reduction function for the branching heuristic scores, min or max. Using max can be better on some models.
#     method: kfsb  # babsr is fast but less accurate; fsb is slow but most accurate; kfsb is usually a balance.
#     candidates: 3  # Number of candidates to consider in fsb and kfsb. More leads to slower but better branching. 3 is typically good enough.
